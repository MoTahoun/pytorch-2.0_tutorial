{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Quick PyTorch 2.0 Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook last updated: 2024-12-12 11:26:42.455214\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(f\"Notebook last updated: {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Getting setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Current PyTorch version: 2.5.1+cu118 (should be 2.X+)\n",
      "[INFO] PyTorch 2.x installed, you'll be able to use the new features.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check pytorch version\n",
    "pt_version = torch.__version__\n",
    "print(f\"[INFO] Current PyTorch version: {pt_version} (should be 2.X+)\")\n",
    "\n",
    "# Install PyTorch 2.0 if necessary\n",
    "if pt_version.split(\".\")[0] == \"1\": # Check if PyTorch version begins with 1 \n",
    "    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "    print(\"[INFO] PyTorch 2.x installed, if you're on Google Colab, you may need to restart your runtime.\\\n",
    "          Though as of April 2023, Google Colab comes with PyTorch 2.0 pre-installed.\")\n",
    "    import torch\n",
    "    pt_version = torch.__version__\n",
    "    print(f\"[INFO] Current PyTorch version: {pt_version} (should be 2.x+)\")\n",
    "else:\n",
    "    print(\"[INFO] PyTorch 2.x installed, you'll be able to use the new features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get GPU info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU name: NVIDIA_GeForce_RTX_3080_Laptop_GPU\n",
      "GPU capability score: (8, 6)\n",
      "GPU score higher than or equal to (8, 0), PyTorch 2.x speedup features available.\n",
      "GPU information:\n",
      "Thu Dec 12 11:26:44 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080 ...    On  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   54C    P3             23W /   90W |     145MiB /  16384MiB |     19%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      3145      G   /usr/lib/xorg/Xorg                            131MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Make sure we're using a NVIDIA GPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_info = !nvidia-smi\n",
    "    gpu_info = '\\n'.join(gpu_info)\n",
    "    if gpu_info.find(\"failed\") >= 0:\n",
    "        print(\"Not connected to a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")\n",
    "        \n",
    "    # Get GPU Name\n",
    "    gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
    "    gpu_name = gpu_name[1]\n",
    "    GPU_NAME = gpu_name.replace(\" \", \"_\")    # remove underscores for easier saving\n",
    "    print(f'GPU name: {GPU_NAME}')\n",
    "    \n",
    "    # Get GPU capability score\n",
    "    GPU_SCORE = torch.cuda.get_device_capability()\n",
    "    print(f\"GPU capability score: {GPU_SCORE}\")\n",
    "    if GPU_SCORE >= (8, 0):\n",
    "        print(f\"GPU score higher than or equal to (8, 0), PyTorch 2.x speedup features available.\")\n",
    "    else:\n",
    "        print(f\"GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\")\n",
    "        \n",
    "    # Print GPU info\n",
    "    print(f\"GPU information:\\n{gpu_info}\")\n",
    "    \n",
    "else:\n",
    "    print(\"PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Globally set devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer weights are on device: cuda:0\n",
      "Layer creating data on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Set the device with context manager (requires PyTorch 2.x+)\n",
    "with torch.device(device):\n",
    "    # All tensors created in this block will be on device\n",
    "    layer = torch.nn.Linear(20, 30)\n",
    "    print(f\"Layer weights are on device: {layer.weight.device}\")\n",
    "    print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now back to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer weights are on device: cpu\n",
      "Layer creating data on device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# Set the device globally\n",
    "torch.set_default_device(\"cpu\")\n",
    "\n",
    "# All tensors created will be on \"cpu\"\n",
    "layer = torch.nn.Linear(20, 30)\n",
    "print(f\"Layer weights are on device: {layer.weight.device}\")\n",
    "print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
